{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import os\n",
    "from mido.midifiles.meta import KeySignatureError\n",
    "from mido import MidiFile\n",
    "import mido\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "from base import BaseModel\n",
    "from datasets import load_rolls, MotifDataset, roll_to_monoroll\n",
    "from models import MonophonicTransformer\n",
    "from utils import roll_to_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 486 rolls with non-4/4 signatures\n",
      "834 rolls were loaded from 910 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_path = 'samples/music/flute/'\n",
    "rolls, tpb = load_rolls(test_path)\n",
    "sn.heatmap(rolls[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452 piano rolls loaded with [00, 01] - 0.973 ± 0.161 maximum consecutive notes\n"
     ]
    }
   ],
   "source": [
    "motif_size = 32\n",
    "bits = 16\n",
    "paths = [\n",
    "    # 'samples/music/giantpiano/',\n",
    "    'samples/music/flute/',\n",
    "    # 'samples/music/jazz/',\n",
    "#     'samples/music/maestro/',\n",
    "    # 'samples/music/mfiles/',\n",
    "    # 'samples/music/midiworld/',\n",
    "]\n",
    "dataset = MotifDataset(paths, motif_size=motif_size, bits=bits, notespbeat=20, monophonic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f109d94f7f0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = dataset[20][0]\n",
    "# sn.heatmap(r)\n",
    "# r = roll_to_monoroll(r)\n",
    "r = [x if x > 0 else np.nan for x in r]\n",
    "plt.plot(range(len(r)), r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = dataset.rolls[88]\n",
    "r.shape\n",
    "dataset.keys[88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network ready 299264 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brendan/OneDrive/Masters/BrainHack2022/brainharmonic/models.py:319: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = torch.tensor(data.transpose(0, 1), dtype=int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0,  0,  0,  0,  0, 55, 59,  0,  0,  0, 66,  0, 67, 59],\n",
      "        [ 0, 67, 65,  0,  0,  0,  0, 62, 62,  0,  0,  0, 66,  0,  0, 59],\n",
      "        [67, 67, 65,  0, 67,  0,  0, 60, 62,  0,  0,  0, 66,  0, 62, 59],\n",
      "        [ 0,  0,  0,  0, 67,  0,  0, 59, 62, 74,  0,  0,  0,  0,  0, 59],\n",
      "        [ 0,  0,  0,  0, 67,  0,  0, 55, 67,  0,  0,  0, 66,  0,  0, 59],\n",
      "        [66,  0, 65,  0, 67, 66,  0, 58, 64,  0,  0,  0,  0,  0, 62, 59],\n",
      "        [66,  0, 65, 57, 67, 66,  0, 57, 64,  0,  0,  0, 64,  0, 65, 59],\n",
      "        [ 0,  0,  0,  0,  0, 66,  0, 55, 64, 71,  0,  0, 64,  0, 70, 59],\n",
      "        [ 0,  0,  0,  0,  0, 66,  0, 55, 59,  0,  0,  0, 64,  0,  0, 59],\n",
      "        [64,  0, 65,  0,  0, 66,  0, 55, 62,  0,  0,  0,  0,  0, 74, 59],\n",
      "        [64,  0, 65,  0, 71,  0,  0, 52, 62,  0,  0,  0,  0,  0,  0, 59],\n",
      "        [64,  0,  0,  0, 67,  0,  0, 50, 62, 76,  0,  0,  0,  0,  0, 59],\n",
      "        [ 0,  0,  0,  0,  0, 66,  0, 52, 62,  0,  0,  0, 64,  0, 77, 59],\n",
      "        [74,  0,  0,  0,  0, 66, 48, 55, 64, 72,  0, 65,  0,  0, 77,  0],\n",
      "        [ 0,  0, 65,  0, 62, 66,  0, 55, 59,  0,  0,  0, 64,  0, 75,  0],\n",
      "        [ 0,  0,  0,  0, 62, 66,  0, 55, 59,  0,  0,  0,  0,  0,  0, 59],\n",
      "        [ 0,  0,  0,  0, 62,  0, 50, 55,  0,  0,  0,  0, 66,  0, 77, 59],\n",
      "        [ 0,  0,  0,  0, 62,  0,  0, 57,  0,  0,  0,  0, 66,  0, 75, 59],\n",
      "        [71,  0, 65,  0,  0,  0,  0, 57, 67,  0,  0,  0, 66,  0,  0, 59],\n",
      "        [71,  0,  0,  0,  0,  0,  0, 57, 64,  0,  0,  0,  0,  0,  0, 59],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0, 55, 64,  0, 46,  0,  0,  0,  0, 59],\n",
      "        [69,  0,  0,  0,  0, 66,  0, 57, 64,  0, 46,  0, 62,  0,  0, 59],\n",
      "        [ 0,  0, 65, 57,  0, 66, 54, 57, 64,  0, 46,  0, 64, 74,  0, 59],\n",
      "        [ 0,  0,  0,  0,  0, 66,  0, 57,  0,  0,  0,  0,  0,  0,  0, 59],\n",
      "        [ 0,  0,  0,  0,  0, 66,  0, 55, 59,  0, 41,  0, 66, 72,  0, 59],\n",
      "        [66,  0,  0,  0,  0, 66,  0, 58, 60,  0,  0,  0, 66,  0,  0, 59],\n",
      "        [66,  0, 65,  0,  0,  0,  0, 57, 59,  0,  0,  0, 66, 74,  0,  0],\n",
      "        [66,  0, 64,  0, 67,  0,  0, 55, 57,  0,  0,  0,  0,  0,  0, 57],\n",
      "        [ 0,  0,  0,  0, 67, 66, 54, 57, 57,  0, 41,  0,  0, 72,  0, 57],\n",
      "        [74, 62,  0,  0,  0, 66, 54, 58, 59, 74, 41, 65, 62,  0,  0, 57],\n",
      "        [ 0,  0, 65,  0,  0, 66, 54, 57, 55,  0, 41,  0, 64,  0,  0,  0],\n",
      "        [ 0, 62,  0,  0, 67, 66, 54, 55,  0,  0,  0,  0,  0,  0,  0, 55]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"LayerNormKernelImpl\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/brendan/OneDrive/Masters/BrainHack2022/brainharmonic/monophonic.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/brendan/OneDrive/Masters/BrainHack2022/brainharmonic/monophonic.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/brendan/OneDrive/Masters/BrainHack2022/brainharmonic/monophonic.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     dataset, batch_size, \u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/brendan/OneDrive/Masters/BrainHack2022/brainharmonic/monophonic.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/brendan/OneDrive/Masters/BrainHack2022/brainharmonic/monophonic.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m val_loader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/brendan/OneDrive/Masters/BrainHack2022/brainharmonic/monophonic.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     dataset, batch_size, num_workers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/brendan/OneDrive/Masters/BrainHack2022/brainharmonic/monophonic.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/brendan/OneDrive/Masters/BrainHack2022/brainharmonic/monophonic.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m net_multi\u001b[39m.\u001b[39;49mfit(train_loader, val_loader, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/OneDrive/Masters/BrainHack2022/brainharmonic/base.py:195\u001b[0m, in \u001b[0;36mBaseModel.fit\u001b[0;34m(self, train_loader, val_loader, epochs, patience, verbose)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval()\n\u001b[1;32m    194\u001b[0m \u001b[39m# Training losses.\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_loss_tr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmini_batch_loop(train_loader)\n\u001b[1;32m    196\u001b[0m \u001b[39m# Validation losses.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_loss_val, best_loss, best_acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmini_batch_loop(\n\u001b[1;32m    198\u001b[0m     val_loader, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    199\u001b[0m )\n",
      "File \u001b[0;32m~/OneDrive/Masters/BrainHack2022/brainharmonic/base.py:92\u001b[0m, in \u001b[0;36mBaseModel.mini_batch_loop\u001b[0;34m(self, data, train)\u001b[0m\n\u001b[1;32m     90\u001b[0m     pred_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\u001b[39m*\u001b[39mx_cuda)\n\u001b[1;32m     91\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     pred_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(x\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice))\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(y, \u001b[39mlist\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(y, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m     94\u001b[0m     y_cuda \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(y_i\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m y_i \u001b[39min\u001b[39;00m y)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/OneDrive/Masters/BrainHack2022/brainharmonic/models.py:323\u001b[0m, in \u001b[0;36mMonophonicTransformer.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mfor\u001b[39;00m i, e_tf \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder):\n\u001b[1;32m    322\u001b[0m     e_tf\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 323\u001b[0m     data \u001b[39m=\u001b[39m e_tf(data, snorm_rel)\n\u001b[1;32m    324\u001b[0m \u001b[39mfor\u001b[39;00m i, d_tf \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder):\n\u001b[1;32m    325\u001b[0m     d_tf\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/OneDrive/Masters/BrainHack2022/brainharmonic/models.py:27\u001b[0m, in \u001b[0;36mSelfAttentionBlock.forward\u001b[0;34m(self, x_in, mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x_in, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 27\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(x_in)\n\u001b[1;32m     28\u001b[0m     \u001b[39mprint\u001b[39m(x)\n\u001b[1;32m     29\u001b[0m     x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\n\u001b[1;32m     30\u001b[0m         query\u001b[39m=\u001b[39mx, key\u001b[39m=\u001b[39mx, value\u001b[39m=\u001b[39mx, attn_mask\u001b[39m=\u001b[39mmask,\n\u001b[1;32m     31\u001b[0m         need_weights\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     32\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[1;32m    190\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2486\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   2483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2484\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[1;32m   2485\u001b[0m     )\n\u001b[0;32m-> 2486\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"LayerNormKernelImpl\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "net_multi = MonophonicTransformer(encoder_depth=128, decoder_depth=128, heads=16)\n",
    "n_param = sum(\n",
    "    p.numel() for p in net_multi.parameters() if p.requires_grad\n",
    ")\n",
    "print('Network ready {:d} parameters'.format(n_param))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset, batch_size, True, num_workers=1\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset, batch_size, num_workers=1\n",
    ")\n",
    "\n",
    "net_multi.fit(train_loader, val_loader, epochs=10, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_multi.save_model('weights/all_multi.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MusicTransformer(\n",
       "  (encoder): ModuleList(\n",
       "    (0): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (1): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (2): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (3): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (4): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (5): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (6): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (7): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (8): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (9): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (10): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (11): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (12): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (13): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (14): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (15): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (16): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (17): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (18): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (19): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (20): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (21): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (22): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (23): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (24): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (25): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (26): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (27): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (28): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (29): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (30): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (31): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (32): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (33): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (34): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (35): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (36): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (37): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (38): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (39): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (40): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (41): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (42): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (43): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (44): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (45): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (46): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (47): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (48): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (49): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (50): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (51): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (52): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (53): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (54): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (55): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (56): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (57): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (58): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (59): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (60): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (61): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (62): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (63): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (64): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (65): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (66): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (67): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (68): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (69): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (70): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (71): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (72): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (73): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (74): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (75): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (76): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (77): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (78): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (79): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (80): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (81): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (82): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (83): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (84): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (85): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (86): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (87): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (88): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (89): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (90): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (91): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (92): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (93): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (94): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (95): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (96): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (97): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (98): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (99): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (100): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (101): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (102): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (103): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (104): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (105): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (106): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (107): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (108): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (109): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (110): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (111): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (112): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (113): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (114): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (115): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (116): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (117): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (118): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (119): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (120): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (121): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (122): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (123): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (124): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (125): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (126): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (127): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (1): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (2): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (3): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (4): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (5): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (6): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (7): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (8): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (9): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (10): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (11): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (12): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (13): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (14): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (15): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (16): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (17): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (18): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (19): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (20): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (21): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (22): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (23): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (24): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (25): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (26): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (27): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (28): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (29): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (30): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (31): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (32): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (33): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (34): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (35): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (36): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (37): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (38): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (39): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (40): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (41): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (42): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (43): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (44): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (45): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (46): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (47): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (48): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (49): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (50): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (51): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (52): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (53): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (54): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (55): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (56): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (57): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (58): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (59): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (60): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (61): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (62): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (63): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (64): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (65): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (66): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (67): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (68): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (69): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (70): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (71): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (72): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (73): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (74): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (75): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (76): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (77): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (78): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (79): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (80): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (81): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (82): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (83): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (84): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (85): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (86): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (87): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (88): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (89): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (90): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (91): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (92): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (93): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (94): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (95): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (96): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (97): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (98): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (99): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (100): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (101): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (102): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (103): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (104): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (105): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (106): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (107): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (108): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (109): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (110): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (111): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (112): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (113): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (114): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (115): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (116): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (117): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (118): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (119): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (120): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (121): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (122): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (123): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (124): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (125): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (126): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (127): SelfAttentionBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_multi = MonophonicTransformer(encoder_depth=128, decoder_depth=128, multitokens=True, heads=16)\n",
    "net_multi.load_state_dict(torch.load('weights/all_multi.pt'))\n",
    "net_multi.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = np.random.randint(0, len(rolls))\n",
    "random_roll = rolls[rand_idx]\n",
    "random_tpb = tpb[rand_idx]\n",
    "random_motif = random_roll[:, :motif_size].astype(np.float32)\n",
    "random_vel = np.mean(random_motif[random_motif > 0])\n",
    "song = random_roll[:, :motif_size + motif_size] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MonophonicTransformer' object has no attribute 'multitokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/brendan/OneDrive/Masters/BrainHack2022/brainharmonic/monophonic.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/brendan/OneDrive/Masters/BrainHack2022/brainharmonic/monophonic.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m p_song, pred_song \u001b[39m=\u001b[39m net_multi\u001b[39m.\u001b[39;49msong((random_motif \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mfloat32), \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/brendan/OneDrive/Masters/BrainHack2022/brainharmonic/monophonic.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m, \u001b[39m18\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/brendan/OneDrive/Masters/BrainHack2022/brainharmonic/monophonic.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/OneDrive/Masters/BrainHack2022/brainharmonic/models.py:350\u001b[0m, in \u001b[0;36mMonophonicTransformer.song\u001b[0;34m(self, motif, n_beats)\u001b[0m\n\u001b[1;32m    348\u001b[0m song \u001b[39m=\u001b[39m [motif]\n\u001b[1;32m    349\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_beats):\n\u001b[0;32m--> 350\u001b[0m     beat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_beat(motif)\n\u001b[1;32m    351\u001b[0m     new_notes \u001b[39m=\u001b[39m deepcopy(beat)\n\u001b[1;32m    352\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultitokens:\n",
      "File \u001b[0;32m~/OneDrive/Masters/BrainHack2022/brainharmonic/models.py:339\u001b[0m, in \u001b[0;36mMonophonicTransformer.next_beat\u001b[0;34m(self, motif)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    336\u001b[0m     tensor_motif \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(\n\u001b[1;32m    337\u001b[0m         np\u001b[39m.\u001b[39mexpand_dims(motif, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    338\u001b[0m     )\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 339\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultitokens:\n\u001b[1;32m    340\u001b[0m         next_beat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m(tensor_motif))\n\u001b[1;32m    341\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1184\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1185\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1186\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MonophonicTransformer' object has no attribute 'multitokens'"
     ]
    }
   ],
   "source": [
    "p_song, pred_song = net_multi.song((random_motif > 0).astype(np.float32), 1)\n",
    "plt.figure(figsize=(12, 18))\n",
    "plt.subplot(3, 1, 1)\n",
    "sn.heatmap(p_song, cbar=False)\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.subplot(3, 1, 2)\n",
    "sn.heatmap(pred_song, cbar=False)\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])\n",
    "plt.subplot(3, 1, 3)\n",
    "sn.heatmap(song, cbar=False)\n",
    "plt.gca().set_xticks([])\n",
    "plt.gca().set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note on 74 127 0\n",
      "Note on 73 127 480\n",
      "Note off 74\n",
      "Note on 69 127 480\n",
      "Note off 73\n",
      "Note on 81 127 480\n",
      "Note off 69\n",
      "Note on 80 127 480\n",
      "Note off 81\n",
      "Note on 76 127 480\n",
      "Note off 80\n",
      "Note on 83 127 480\n",
      "Note off 76\n",
      "Note on 81 127 480\n",
      "Note off 83\n",
      "Note on 78 127 480\n",
      "Note off 81\n",
      "Note on 76 127 480\n",
      "Note off 78\n",
      "Note on 69 127 960\n",
      "Note off 76\n",
      "Note on 74 127 480\n",
      "Note off 69\n",
      "Note on 73 127 480\n",
      "Note off 74\n",
      "Note on 69 127 480\n",
      "Note off 73\n",
      "Note on 81 127 480\n",
      "Note off 69\n",
      "Note on 80 127 480\n",
      "Note off 81\n",
      "Note on 76 127 480\n",
      "Note off 80\n",
      "Note on 83 127 480\n",
      "Note off 76\n",
      "Note on 81 127 480\n",
      "Note off 83\n",
      "Note on 78 127 480\n",
      "Note off 81\n",
      "Note on 76 127 480\n",
      "Note off 78\n",
      "Note on 69 127 960\n",
      "Note off 76\n",
      "Note on 78 127 480\n",
      "Note off 69\n",
      "Note on 74 127 480\n",
      "Note off 78\n",
      "Note on 71 127 480\n",
      "Note off 74\n",
      "Note on 76 127 480\n",
      "Note off 71\n",
      "Note on 73 127 480\n",
      "Note off 76\n",
      "Note on 69 127 480\n",
      "Note off 73\n",
      "Note on 78 127 480\n",
      "Note off 69\n",
      "Note on 74 127 480\n",
      "Note off 78\n",
      "Note on 60 127 480\n",
      "Note on 67 127 0\n",
      "Note on 98 127 0\n",
      "Note off 74\n",
      "Note on 73 127 480\n",
      "Note off 60\n",
      "Note off 67\n",
      "Note off 98\n",
      "Note off 73 480\n",
      "Note on 60 127 960\n",
      "Note on 126 127 480\n",
      "Note off 60\n",
      "Note on 60 127 480\n",
      "Note off 126\n",
      "Note on 67 127 480\n",
      "Note on 73 127 0\n",
      "Note off 60\n",
      "Note on 60 127 480\n",
      "Note off 67\n",
      "Note off 73\n",
      "Note on 24 127 480\n",
      "Note off 60\n",
      "Note on 60 127 480\n",
      "Note off 24\n",
      "Note off 60 960\n",
      "Note on 67 127 1440\n",
      "Note on 98 127 0\n",
      "Note off 67 480\n",
      "Note off 98 480\n",
      "Note on 60 127 1440\n",
      "Note on 126 127 480\n",
      "Note off 60\n",
      "Note off 126 480\n",
      "Note on 67 127 960\n",
      "Note on 73 127 0\n",
      "Note on 60 127 480\n",
      "Note off 67\n",
      "Note off 73\n",
      "Note off 60 480\n",
      "Note on 60 127 960\n",
      "Note off 60 960\n",
      "Note on 67 127 1920\n",
      "Note on 98 127 0\n",
      "Note off 67 480\n",
      "Note off 98 480\n",
      "Note on 60 127 960\n",
      "Note off 60 480\n",
      "Note on 60 127 1920\n",
      "Note on 67 127 0\n",
      "Note on 83 127 0\n",
      "Note on 98 127 0\n"
     ]
    }
   ],
   "source": [
    "tmpname = \"/tmp/tmpmidi.mid\"\n",
    "roll_to_midi(os.path.dirname(tmpname), os.path.basename(tmpname), pred_song, random_tpb)\n",
    "# mid = MidiFile(tmpname)\n",
    "# port = mido.open_output('Midi Through Port-0')\n",
    "# for msg in mid.play():\n",
    "#     port.send(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_song"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
